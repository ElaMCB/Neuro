Elena Mereanu
Atlanta Metropolitan Area • 770.377.0765 • elena.mereanu@gmail.com • linkedin.com/in/elenamereanu • https://ElaMCB.github.io
QA Lead — Prompt Engineer (transition-focused)

PROFESSIONAL SUMMARY
Prompt-focused QA Lead and SDET with 10+ years building validation systems, automation frameworks, and production reliability programs. Experienced in prompt engineering, LLM testing, and LLMOps—designing evaluation pipelines, automating data and model validation, and integrating model checks into CI/CD. Proven ability to translate QA/SDET engineering practices into robust AI validation workflows that improve reliability and speed of deployment.

CORE COMPETENCIES
Prompt Engineering • LLMOps & Model Evaluation • AI System Validation • Test Automation • MLOps Integration
Data Validation • Model Monitoring & Drift Detection • CI/CD Integration • Python • Test Framework Architecture

KEY ACHIEVEMENTS
- Improved model-accuracy validation efficiency by 30% by implementing automated LLM evaluation pipelines and metric-driven acceptance tests.
- Increased test development throughput by ~25% by introducing prompt-engineered test generation and AI-assisted test scripting.
- Integrated AI-driven static analysis and anomaly detection into CI/CD, reducing production regressions and improving incident triage.

PROFESSIONAL EXPERIENCE
Transport4 — Alpharetta, GA
QA Lead • 01/2015 – 09/2025
- Led QA for web and mobile products while embedding AI-first validation practices across feature lifecycles, prioritizing production reliability for LLM-enabled capabilities.
- Designed and implemented automated model evaluation pipelines: dataset validation, unit/functional prompt tests, metrics collection (accuracy, latency, hallucination checks), and automated gating in CI/CD.
- Developed prompt engineering workflows and scenario-generation agents to stress-test LLM responses for edge cases and adversarial inputs; reduced post-release incidents for AI features.
- Built and maintained LLMOps processes: model versioning, automated evaluation runs, rollback criteria, and monitoring hooks for drift and performance regression.
- Introduced AI-powered test generation and prompt templates to accelerate test coverage creation; coached team on prompt design patterns and evaluation metrics.
- Integrated Playwright for cross-browser functional testing and expanded automation to include API-based model validation and synthetic data generation.
- Collaborated with data engineers and ML teams to define validation criteria and thresholds, enabling repeatable model acceptance tests.

Softwin Technologies Pvt. Ltd. — Remote
Sr. QA Engineer / QA Lead • 06/2014 – 12/2014
- Implemented CI/CD quality gates and automated checks to reduce deployment errors and improve release confidence.
- Led integration testing, data validation, and complex cross-system test efforts; prioritized observability and traceability in test results.

PROJECTS (select)
Neuro — intent-driven programming language & tooling (https://github.com/ElaMCB/Neuro)
- Contributed language/tooling used to convert high-level intents into reproducible evaluation pipelines and validation automation.

AI Job Search System (personal project)
- Built a retrieval-augmented system demonstrating prompt engineering patterns, evaluation harnesses, and production-minded monitoring for LLM components.

TECHNICAL SKILLS
Languages & Frameworks: Python, TypeScript, Playwright, PyTorch (familiar)
Tools & Platforms: ChatGPT, Claude, OpenAI API, JMeter, Playwright, Selenium, Postman
Cloud & Infra: AWS (EC2, S3, RDS), Docker, Kubernetes, Azure DevOps, GitHub, CircleCI
Databases: MS SQL Server, DBeaver
Methodologies: Agile (Scrum/Kanban), AI-Driven Development Lifecycles, LLMOps

EDUCATION
M.S., Quality Assurance — Kennesaw State University
Cybersecurity and Mobility — University System of Georgia

CERTIFICATIONS (selected)
- AWS Certified Machine Learning — AWS
- NLP Specialization — Stanford / DeepLearning.AI
- Automated Testing for LLMOps — DeepLearning.AI
- PCPP2 — Python Institute
- ISTQB Advanced Level Test Automation Engineer

KEY ACHIEVEMENTS
- Improved model-accuracy validation efficiency by 30% by implementing automated LLM evaluation pipelines and metric-driven acceptance tests.
- Increased test development throughput by ~25% by introducing prompt-engineered test generation and AI-assisted test scripting.
- Integrated AI-driven static analysis and anomaly detection into CI/CD, reducing production regressions and improving incident triage.

PROFESSIONAL EXPERIENCE
Transport4 — Alpharetta, GA
QA Lead • 01/2015 – 09/2025
- Led QA for web and mobile products while embedding AI-first validation practices across feature lifecycles, prioritizing production reliability for LLM-enabled capabilities.
- Designed and implemented automated model evaluation pipelines: dataset validation, unit/functional prompt tests, metrics collection (accuracy, latency, hallucination checks), and automated gating in CI/CD.
- Developed prompt engineering workflows and scenario-generation agents to stress-test LLM responses for edge cases and adversarial inputs; reduced post-release incidents for AI features.
- Built and maintained LLMOps processes: model versioning, automated evaluation runs, rollback criteria, and monitoring hooks for drift and performance regression.
- Introduced AI-powered test generation and prompt templates to accelerate test coverage creation; coached team on prompt design patterns and evaluation metrics.
- Integrated Playwright for cross-browser functional testing and expanded automation to include API-based model validation and synthetic data generation.
- Collaborated with data engineers and ML teams to define validation criteria and thresholds, enabling repeatable model acceptance tests.

Softwin Technologies Pvt. Ltd. — Remote
Sr. QA Engineer / QA Lead • 06/2014 – 12/2014
- Implemented CI/CD quality gates and automated checks to reduce deployment errors and improve release confidence.
- Led integration testing, data validation, and complex cross-system test efforts; prioritized observability and traceability in test results.

PROJECTS (select)
Neuro — intent-driven programming language & tooling (https://github.com/ElaMCB/Neuro)
- Contributed language/tooling used to convert high-level intents into reproducible evaluation pipelines and validation automation.

AI Job Search System (personal project)
- Built a retrieval-augmented system demonstrating prompt engineering patterns, evaluation harnesses, and production-minded monitoring for LLM components.

TECHNICAL SKILLS
Languages & Frameworks: Python, TypeScript, Playwright, PyTorch (familiar)
Tools & Platforms: ChatGPT, Claude, OpenAI API, JMeter, Playwright, Selenium, Postman
Cloud & Infra: AWS (EC2, S3, RDS), Docker, Kubernetes, Azure DevOps, GitHub, CircleCI
Databases: MS SQL Server, DBeaver
Methodologies: Agile (Scrum/Kanban), AI-Driven Development Lifecycles, LLMOps

EDUCATION
M.S., Quality Assurance — Kennesaw State University
Cybersecurity and Mobility — University System of Georgia

CERTIFICATIONS (selected)
- AWS Certified Machine Learning — AWS
- NLP Specialization — Stanford / DeepLearning.AI
- Automated Testing for LLMOps — DeepLearning.AI
- PCPP2 — Python Institute
- ISTQB Advanced Level Test Automation Engineer

Key focus: Prompt Engineering, LLMOps, Model Evaluation, RAG, Data Validation, Model Monitoring.

CORE COMPETENCIES
Prompt Engineering • LLMOps & Model Evaluation • AI System Validation • Test Automation • MLOps Integration
Data Validation • Model Monitoring & Drift Detection • CI/CD Integration • Python • Test Framework Architecture

KEY ACHIEVEMENTS
- Improved model-accuracy validation efficiency by 30% by implementing automated LLM evaluation pipelines and metric-driven acceptance tests.
- Increased test development throughput by ~25% by introducing prompt-engineered test generation and AI-assisted test scripting.
- Integrated AI-driven static analysis and anomaly detection into CI/CD, reducing production regressions and improving incident triage.

PROFESSIONAL EXPERIENCE
Transport4 — Alpharetta, GA
QA Lead • 01/2015 – 09/2025
- Led QA for web and mobile products while embedding AI-first validation practices across feature lifecycles, prioritizing production reliability for LLM-enabled capabilities.
- Designed and implemented automated model evaluation pipelines: dataset validation, unit/functional prompt tests, metrics collection (accuracy, latency, hallucination checks), and automated gating in CI/CD.
- Developed prompt engineering workflows and scenario-generation agents to stress-test LLM responses for edge cases and adversarial inputs; reduced post-release incidents for AI features.
- Built and maintained LLMOps processes: model versioning, automated evaluation runs, rollback criteria, and monitoring hooks for drift and performance regression.
- Introduced AI-powered test generation and prompt templates to accelerate test coverage creation; coached team on prompt design patterns and evaluation metrics.
- Integrated Playwright for cross-browser functional testing and expanded automation to include API-based model validation and synthetic data generation.
- Collaborated with data engineers and ML teams to define validation criteria and thresholds, enabling repeatable model acceptance tests.

Softwin Technologies Pvt. Ltd. — Remote
Sr. QA Engineer / QA Lead • 06/2014 – 12/2014
- Implemented CI/CD quality gates and automated checks to reduce deployment errors and improve release confidence.
- Led integration testing, data validation, and complex cross-system test efforts; prioritized observability and traceability in test results.

PROJECTS (select)
Neuro — intent-driven programming language & tooling (https://github.com/ElaMCB/Neuro)
- Contributed language/tooling used to convert high-level intents into reproducible evaluation pipelines and validation automation.

AI Job Search System (personal project)
- Built a retrieval-augmented system demonstrating prompt engineering patterns, evaluation harnesses, and production-minded monitoring for LLM components.

TECHNICAL SKILLS
Languages & Frameworks: Python, TypeScript, Playwright, PyTorch (familiar)
Tools & Platforms: ChatGPT, Claude, OpenAI API, JMeter, Playwright, Selenium, Postman
Cloud & Infra: AWS (EC2, S3, RDS), Docker, Kubernetes, Azure DevOps, GitHub, CircleCI
Databases: MS SQL Server, DBeaver
Methodologies: Agile (Scrum/Kanban), AI-Driven Development Lifecycles, LLMOps

EDUCATION
M.S., Quality Assurance — Kennesaw State University
Cybersecurity and Mobility — University System of Georgia

CERTIFICATIONS (selected)
- AWS Certified Machine Learning — AWS
- NLP Specialization — Stanford / DeepLearning.AI
- Automated Testing for LLMOps — DeepLearning.AI
- PCPP2 — Python Institute
- ISTQB Advanced Level Test Automation Engineer

KEY ACHIEVEMENTS
- Improved model-accuracy validation efficiency by 30% by implementing automated LLM evaluation pipelines and metric-driven acceptance tests.
- Increased test development throughput by ~25% by introducing prompt-engineered test generation and AI-assisted test scripting.
- Integrated AI-driven static analysis and anomaly detection into CI/CD, reducing production regressions and improving incident triage.

PROFESSIONAL EXPERIENCE
Transport4 — Alpharetta, GA
QA Lead • 01/2015 – 09/2025
- Led QA for web and mobile products while embedding AI-first validation practices across feature lifecycles, prioritizing production reliability for LLM-enabled capabilities.
- Designed and implemented automated model evaluation pipelines: dataset validation, unit/functional prompt tests, metrics collection (accuracy, latency, hallucination checks), and automated gating in CI/CD.
- Developed prompt engineering workflows and scenario-generation agents to stress-test LLM responses for edge cases and adversarial inputs; reduced post-release incidents for AI features.
- Built and maintained LLMOps processes: model versioning, automated evaluation runs, rollback criteria, and monitoring hooks for drift and performance regression.
- Introduced AI-powered test generation and prompt templates to accelerate test coverage creation; coached team on prompt design patterns and evaluation metrics.
- Integrated Playwright for cross-browser functional testing and expanded automation to include API-based model validation and synthetic data generation.
- Collaborated with data engineers and ML teams to define validation criteria and thresholds, enabling repeatable model acceptance tests.

Softwin Technologies Pvt. Ltd. — Remote
Sr. QA Engineer / QA Lead • 06/2014 – 12/2014
- Implemented CI/CD quality gates and automated checks to reduce deployment errors and improve release confidence.
- Led integration testing, data validation, and complex cross-system test efforts; prioritized observability and traceability in test results.

PROJECTS (select)
Neuro — intent-driven programming language & tooling (https://github.com/ElaMCB/Neuro)
- Contributed language/tooling used to convert high-level intents into reproducible evaluation pipelines and validation automation.

AI Job Search System (personal project)
- Built a retrieval-augmented system demonstrating prompt engineering patterns, evaluation harnesses, and production-minded monitoring for LLM components.

TECHNICAL SKILLS
Languages & Frameworks: Python, TypeScript, Playwright, PyTorch (familiar)
Tools & Platforms: ChatGPT, Claude, OpenAI API, JMeter, Playwright, Selenium, Postman
Cloud & Infra: AWS (EC2, S3, RDS), Docker, Kubernetes, Azure DevOps, GitHub, CircleCI
Databases: MS SQL Server, DBeaver
Methodologies: Agile (Scrum/Kanban), AI-Driven Development Lifecycles, LLMOps

EDUCATION
M.S., Quality Assurance — Kennesaw State University
Cybersecurity and Mobility — University System of Georgia

CERTIFICATIONS (selected)
- AWS Certified Machine Learning — AWS
- NLP Specialization — Stanford / DeepLearning.AI
- Automated Testing for LLMOps — DeepLearning.AI
- PCPP2 — Python Institute
- ISTQB Advanced Level Test Automation Engineer
